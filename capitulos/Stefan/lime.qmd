---
title: "Variable importance"
editor: 
  markdown: 
    wrap: sentence
---

# Aplicação LIME

## LIME

#### **Local**

-   Instance-based

#### **Interpretable**

-   Understandable to humans

#### Model-Agnostic

-   Works for any model

#### Explanations

-   Explains the models output

::: footer

Ribeiro, Marco Tulio, et al. "'Why Should I Trust You?'

:   Explaining the Predictions of Any Classifier." *ArXiv.org*, 16 Feb. 2016, [arxiv.org/abs/1602.04938](arxiv.org/abs/1602.04938).
:::

## Motivation

-   Explain what led the ML model to give a certain prediction to an instance

-   What variables affected the decision?

-   Observe the behavior of the ML model with points around the instance of interest

-   Simulate points in the neighborhood

-   Create an interpretable surrogate model to explain the behaviour of the ML model around the instance

    -   Linear Regression, Logistic Regression, GLM, GMA, Decision Tree, etc.

## Motivation

. . .

```{r echo=F, out.width="100%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem3.png")
```

::: footer
"Understanding LIME \| Explainable AI." *Youtube*, [www.youtube.com/watch?v=CYl172IwqKs](www.youtube.com/watch?v=CYl172IwqKs).
:::

## Step 1

Select an instance of interest

. . .

```{r echo=F, out.width="50%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem1.png")
```

::: footer
"Understanding LIME \| Explainable AI." *Youtube*, [www.youtube.com/watch?v=CYl172IwqKs](www.youtube.com/watch?v=CYl172IwqKs).
:::

## Step 2

Generate new instances around the original instance and calculate their result with the ML model

. . .

```{r echo=F, out.width="50%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem4.png")
```

::: footer
"Understanding LIME \| Explainable AI." *Youtube*, [www.youtube.com/watch?v=CYl172IwqKs](www.youtube.com/watch?v=CYl172IwqKs).
:::

## Step 3

Apply a weight to the new instances depending on the distance to the original instance

. . .

```{r echo=F, out.width="50%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem5.png")
```

::: footer
"Understanding LIME \| Explainable AI." *Youtube*, [www.youtube.com/watch?v=CYl172IwqKs](www.youtube.com/watch?v=CYl172IwqKs).
:::

## Step 4

Obtain a new interpretable model with the weighted instances

. . .

```{r echo=F, out.width="40%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem6.png")
```

::: footer
"Understanding LIME \| Explainable AI." *Youtube*, [www.youtube.com/watch?v=CYl172IwqKs](www.youtube.com/watch?v=CYl172IwqKs).
:::

## Step 5

Interpret the local model

. . .

```{r echo=F, out.width="10%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem7.png")
```

```{r echo=F, out.width="60%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem8.png")
```

::: footer
"Understanding LIME \| Explainable AI." *Youtube*, [www.youtube.com/watch?v=CYl172IwqKs](www.youtube.com/watch?v=CYl172IwqKs).
:::

## Obtaining the interpretable model

```{r echo=F, out.width="60%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem9.png")
```

$L(f,g,π_x)$ is a loss function, usually MSE, between the prediction of the ML model and the prediction of the simplified model

$\pi_x(z)$ is a function that weights the new instances according to their distance to the original instance

$\Omega(g)$ penalizes high complexity models

## Aplications

Can be used for classification or regression models

1.  Tabular data

    1.  Values usually sampled from a normal distribution for each variable

2.  Text or Image data

    1.  Words or mega-pixels turned on or off according to a bernoulli distribution

## Aplications in R

```{r echo=F, message=FALSE}
# Loading necessary packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load("lime","MASS", "randomForest", "caret", "e1071")
#Organizing the data
biopsy$ID <- NULL
names(biopsy) <- c('clump thickness', 'uniformity cell size',
'uniformity cell shape', 'marginal adhesion','single epithelial cell size',
'bare nuclei', 'bland chromatin', 'normal nucleoli', 'mitoses','class')
biopsy <- na.omit(biopsy)
```

We shall analyse the "Biopsy" database, which is part of the "MASS" package

```{r echo=F, out.width="40%", fig.align='center'}
knitr::include_graphics("capitulos/Stefan/Imagens/Imagem10.png")
```

::: footer
"LIME \| Machine Learning Model Interpretability Using LIME in R." *Analytics Vidhya*, 18 Jan. 2021, www.analyticsvidhya.com/blog/2021/01/ml-interpretability-using-lime-in-r/.
:::

## Aplications in R

```{r message=FALSE, warning=FALSE}
## 75% of the sample size

smp_size <- floor(0.75 * nrow(biopsy))

## set the seed
set.seed(123)
train_ind <- sample(seq_len(nrow(biopsy)), size = smp_size)

train_biopsy <- biopsy[train_ind, ]
test_biopsy <- biopsy[-train_ind, ]

model_rf <- caret::train(class ~ ., data = train_biopsy,method = "rf", #random forest
trControl = trainControl(method = "repeatedcv", number = 10,repeats = 5, verboseIter = FALSE))
```

::: footer
"LIME \| Machine Learning Model Interpretability Using LIME in R." *Analytics Vidhya*, 18 Jan. 2021, www.analyticsvidhya.com/blog/2021/01/ml-interpretability-using-lime-in-r/.
:::

## Aplications in R

```{r warning=FALSE}
model_rf
```

::: footer
"LIME \| Machine Learning Model Interpretability Using LIME in R." *Analytics Vidhya*, 18 Jan. 2021, www.analyticsvidhya.com/blog/2021/01/ml-interpretability-using-lime-in-r/.
:::

## Aplications in R

```{r warning=FALSE}
biopsy_rf_pred <- predict(model_rf, test_biopsy)
explainer <- lime(train_biopsy, model_rf)
explanation1 <- explain(test_biopsy[93, ], explainer, n_labels = 1, n_features = 9)
plot_features(explanation1)
```

::: footer
"LIME \| Machine Learning Model Interpretability Using LIME in R." *Analytics Vidhya*, 18 Jan. 2021, www.analyticsvidhya.com/blog/2021/01/ml-interpretability-using-lime-in-r/.
:::

## Limitations

-   Difficulties definying the neighborhood

-   Dificulties with non-linearity

-   Incorrect sampling for new instances can cause improbable instances

-   Explanations to close points may vary greatly

-   Easily manipulable to hide biases

::: footer
Alvarez-Melis, David, and Tommi S. Jaakkola.
"On the robustness of interpretability methods." arXiv preprint arXiv:1806.08049 (2018).

Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
"Fooling lime and shap: Adversarial attacks on post hoc explanation methods." In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180-186 (2020).
:::
