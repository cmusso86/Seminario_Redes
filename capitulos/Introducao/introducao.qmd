---
title: "Variable importance"
---

## What to expect 

- Introduction 
  - Interpretability of models
- Methods for model interpretation
- Hands-on example:
  - SHAP
  - LIME
  - Causal Inference
- Conclusions

# Introduction


## Introduction

- Interpretability in a Data Driven World

- Sometimes you do not care why a decision was made. In other cases, knowing the ‘why’ is important. 

- Human desire to find meaning in the world.

- Machine learning model is faster and cheaper to train in comparison to humans.


## Introduction

- If you focus only on performance, you will automatically get more and more opaque models. 

- By default, machine learning models pick up biases from the training data.

- ML can be debugged and audited if interpretable.

```{r echo=F}
knitr::include_graphics("capitulos/Introducao/img/iml.png")
```


## Introduction: Importance of variables

- It is also essential to verify if the accuracy does not result from the exploitation of artifacts.

- Key ingredient of a robust validation procedure.

- Important in applications such in health ans social sciences and where accountability is important. 

- Tool for exploration and analysis in the sciences,  to extract new insights from complex physical, chemical, or biological systems.


## Introduction: Interpretability

- The degree to which a human can understand the cause of a decision or  can consistently predict the model’s result.

- Interpretable ML: “extraction of relevant knowledge from a machine-learning model concerning relationships either contained in data or learned by the model”.

- Explain the predictions: relate the feature values of an instance to its model prediction in a humanly understandable way

## Introduction: The classic approach

- Models inherently interpretable: linear regression, logistic regression, decision tree...
   - Coefficients: rates, odds ratio...
   
- Lower predictive performance in comparison to other machine learning models.

- However, insights are hidden in increasingly complex models. 


## Introduction: Interpreting DNNs

-  Interpreting deep networks remains a young and emerging field of research. 
  - Numerous coexisting approaches.
  - We will present some possible techniques. 
  - Focus on supervised learning.

- [Interpretable Machine Learning, Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)
- [Montavon et al. (2018)](https://www.sciencedirect.com/science/article/pii/S1051200417302385?via%3Dihub)


