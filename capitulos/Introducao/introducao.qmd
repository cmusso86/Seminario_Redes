---
title: "Variable importance"
---

## What to expect

-   Introduction
    -   Interpretability of models
-   Methods for model interpretation
-   Hands-on example:
    -   SHAP
    -   LIME
    -   Causal Inference
-   Conclusions

# Introduction

## Introduction: Variable importance

-   Interpretability in a Data Driven World

-   Sometimes you do not care why a decision was made. In other cases, knowing the 'why' is important.

-   Human desire to find meaning in the world.


## Introduction

-   Performance vs opaque models.

-   ML models pick up biases from the training data.

-   ML can be debugged and audited *if interpretable*.

. . . 

```{r, echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Introducao/img/iml.png")
```

## Introduction: Interpretability

-   Verify if the accuracy results come from artifacts: validation.

-   Important in health and social sciences: accountability.

-   Exploration and analysis in the sciences: extract insights from complex systems.

-   The degree to which a human can understand the cause of a decision or can consistently predict the model's result.


## Introduction: The classic approach

-   Models inherently interpretable: linear regression, logistic regression, decision tree...

    -   Coefficients: rates, odds ratio...

-   Lower predictive performance in comparison to other machine learning models.

-   However, insights are hidden in increasingly complex models.

## Introduction: Interpreting DNNs

-   Interpreting deep networks remains a young and emerging field of research.

-   Numerous coexisting approaches.

-   We will present some possible techniques.

-   Focus on supervised learning.

-   [Interpretable Machine Learning, Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)

-   [Montavon *et al*. (2018)](https://www.sciencedirect.com/science/article/pii/S1051200417302385?via%3Dihub)

- [NeuralNetTools: Beck (2018)](doi: 10.18637/jss.v085.i11).
