---
title: "Variable importance"
editor: 
  markdown: 
    wrap: sentence
---

# Methods for model interpretation

## Classic approach

**Subset selection:**

-   Best, Foward, Backward SW.
-   Mallows' Cp, adjusted RÂ², AIC, BIC.

. . .

```{r echo=F}
knitr::include_graphics("capitulos/Carol/img/step.webp")
```

## Classic approach

**Shinkage:**

-   Ridge, Lasso

. . .

```{r echo=F, out.width="70%", fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/lasso.webp")
```

## Classic approach

**Dimension Reduction:**

-   PCA...

. . .

```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/pca.gif")
```

## Other techniques

-   **Model Agnostic:**
    -   Global: PDP, ALE, Feature Interaction ...
    -   Local: LIME, SHAP

-   **DNN**: Leanerd features, Saliency Maps, Sensitivity Analysis, Taylos Decomposition, LRP

## Interpretability Methods layer


```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/big-picture.png")
```


## Package NeuralNet Tools

```{r warning=FALSE, eval=F}
pacman::p_load(NeuralNetTools, tidyverse,
               nycflights13, nnet)

tomod <- flights %>% 
  filter(month == 12 & carrier == "UA") %>%
       select(arr_delay, dep_delay, dep_time, 
              arr_time, air_time, distance) %>% 
  mutate_each(funs(scale), -arr_delay) %>%
  mutate_each(funs(as.numeric), -arr_delay) %>%
  mutate(arr_delay = scales::rescale(arr_delay, to = c(0, 1))) %>%
  data.frame()

mod <- nnet(arr_delay ~ ., size = 5,
            linout = TRUE, data = tomod,
               trace = FALSE)

plotnet(mod)
```
. . . 

```{r, echo=F, warning=FALSE}
pacman::p_load(NeuralNetTools, tidyverse,
               nycflights13, nnet)

tomod <- flights %>% 
  filter(month == 12 & carrier == "UA") %>%
       select(arr_delay, dep_delay, dep_time, 
              arr_time, air_time, distance) %>% 
  mutate_each(funs(scale), -arr_delay) %>%
  mutate_each(funs(as.numeric), -arr_delay) %>%
  mutate(arr_delay = scales::rescale(arr_delay, to = c(0, 1))) %>%
  data.frame()

mod <- nnet(arr_delay ~ ., size = 5,
            linout = TRUE, data = tomod,
               trace = FALSE)

plotnet(mod)
```


::: footer
[NeuralNetTools: Visualization and Analysis Tools for Neural Networks](doi: 10.18637/jss.v085.i11), *Marcus W. Beck (2018)*.
::: 

## Model-Agnostic Methods: PFI

**Permutation feature importance**

- Permuted the feature's values: breaks relationship between feature and outcome.

- A feature is "important" if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction.


. . . 

```{r echo=F, fig.align='center', out.width="70%"}
knitr::include_graphics("capitulos/Carol/img/perm.jpeg")
```

::: footer
[ll Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously](https://arxiv.org/abs/1801.01489), *Fisher et al (2018)*.
::: 

## Neural Networks Methods

- NN learn features in hidden layers:  special tools to uncover them.
- The gradient can be utilized for more computationally efficient than model-agnostic methods that look at the model "from the outside".
- Learned features, Saliency Maps, Layer-wise relevance propagation (LRP), Influential Instances

## Learned Features: 

- What features has the neural network learned?

- Activation maximization (AM): the input that maximizes the activation of that unit.

. . . 

```{r echo=F, fig.align='center', out.width="70%"}
knitr::include_graphics("capitulos/Carol/img/units.jpg")
```


::: footer
[Interpretable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/), *Christoph Molnar (2023)*.
::: 

## Learned features

- Network dissection

. . . 

```{r echo=F, fig.align='center', out.width="100%"}
knitr::include_graphics("capitulos/Carol/img/dissection-network.png")
```


::: footer
[Interpretable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/), *Christoph Molnar (2023)*.
::: 

## Pixel Attribution: 

- Sensitivity A., Taylor Decom., Saliency Maps
- How did each pixel contribute to a particular prediction?
- **Perturbation-based:** SHAP, LIME
- **Gradient-based:** Vanilla Grad., DeconvNet, Grad-CAM

. . . 

```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/boat.png")
```


::: footer
[Interpretable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/), *Christoph Molnar (2023)*.
::: 
## Pixel Attribution (Saliency Maps): 

```{r echo=F, fig.align='center', out.width="70%"}
knitr::include_graphics("capitulos/Carol/img/original.png")
```

. . . 

```{r echo=F, fig.align='center', out.width="70%"}
knitr::include_graphics("capitulos/Carol/img/saliency.png")
```


::: footer
[Interpretable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/), *Christoph Molnar (2023)*.
::: 
## Layer-wise relevance propagation

-  LRP: Backward propagation technique

. . . 

```{r echo=F, fig.align='center', out.width="70%"}
knitr::include_graphics("capitulos/Carol/img/lrp.png")
```
 
::: footer
[Methods for interpreting and understanding deep neural networks](https://doi.org/10.1016/j.dsp.2017.10.011), *Montavon et al (2018)*.
:::
