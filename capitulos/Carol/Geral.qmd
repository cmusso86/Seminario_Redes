---
title: "Variable importance"
editor: 
  markdown: 
    wrap: sentence
---

# Methods for model interpretation

## Classic approach

**Subset selection:**

-   Best, Foward, Backward SW.
-   Mallows' Cp, adjusted R², AIC, BIC.

. . .

```{r echo=F}
knitr::include_graphics("capitulos/Carol/img/step.webp")
```

## Classic approach

**Shinkage:**

-   Ridge, Lasso

. . .

```{r echo=F, out.width="70%", fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/lasso.webp")
```

## Classic approach

**Dimension Reduction:**

-   PCA...

. . .

```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/pca.gif")
```

## Other techniques

-   **Model Agnostic:**
    -   Global: PDP, ALE, Feature Interaction
    -   Local: LIME, SHAP
-   **Example based**: Counterfactual, Prototypes, KNN
-   **DNN**: Leanerd features, Saliency Maps, Sensitivity Analysis, Taylos Decomposition, LRP

## Interpretability Methods layer


```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/big-picture.png")
```


## Model-Agnostic Methods: PDP

**The partial dependence plot**

- Global, one or two variables. 

- Average marginal effect

- Features are independent

. . .  

```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/pdp.jpeg")
```


## Model-Agnostic Methods: ALE

**Accumulated local effect plots** 

- Dependent features
- Based on the conditional distribution: differences in predictions instead of averages
 
. . . 

```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/ale.jpeg")
```

 
## Model-Agnostic Methods: FI

**Feature interaction**

- “The whole is greater than the sum of its parts”.

- H-statistic quantifies interaction strength: the  joint effects of the features.

. . . 

```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/fi.png")
```


## Model-Agnostic Methods: PFI

**Permutation feature importance**

- Permuted the feature's values: breaks relationship between feature and outcome.

- A feature is "important" if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction.

- [Fisher *et al.* (2018)](https://arxiv.org/abs/1801.01489)

. . . 

```{r echo=F, fig.align='center'}
knitr::include_graphics("capitulos/Carol/img/perm.jpeg")
```


## Neural Networks Methods

- NN learn features in hidden layers:  special tools to uncover them.
- The gradient can be utilized for more computationally efficient than model-agnostic methods that look at the model "from the outside".
- Learned features, Saliency Maps, Layer-wise relevance propagation (LRP), Influential Instances

## Learned Features: 

- What features has the neural network learned?

- Activation maximization (AM)

- Feature Visualization. 


## Pixel Attribution (Saliency Maps): 

- How did each pixel contribute to a particular prediction?
Concepts: Which more abstract concepts has the neural network learned?


## Adversarial Examples 

- are closely related to counterfactual explanations: How can we trick the neural network?

## Influential Instances 
- is a more general approach with a fast implementation for gradient-based methods such as neural networks: How influential was a training data point for a certain prediction?
